---
title: "Flora Quadrats"
author: "DBCA"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    fig_width: 10
    fig_height: 6
    code_folding: hide
    theme: lumen
  pdf_document:
    latex_engine: xelatex
  word_document: default
---
<!--  
This Rmd template provides an example workflow to access submissions and 
attachments of a given ODK Central form using its OData API and disseminate
data, attachments, and this report via CKAN or Google Drive.

You will need:
* An ODK Central form's OData Service URL (ending in .svc), accessed from the 
"Analyse using OData" button on the form submission tab.
* Username and password of an ODK Central webuser authorised to view this form.
* Optional: base URL and write-permitted API key for a CKAN instance.
* Optional: a Google account and the means to authenticate (e.g. phone for 2FA).
-->

<!--
Step 1: Setup ruODK, ckanr
Secrets are set as environment variables to keep them outside of this document. 
Read vignette "setup" for further information.
-->
<!-- 1a. Open the .Renviron file:  -->
```{r edit_renviron, echo=FALSE, eval=FALSE}
usethis::edit_r_environ()
```

<!-- 1b. Paste the following block using your own, working credentials:  -->
```{r paste_env_vars, echo=FALSE, eval=FALSE}
# ODK Central web user with "manager" role on project
ODKC_UN="my@email.com"
ODKC_PW="my-odkc-password"

# CKAN user with permissions to create a dataset
CKAN_URL="https://demo.ckan.org"
CKAN_KEY="my-ckan-api-key"
```
<!-- 1c: Restart the R session to load these environment variables. -->

<!-- 
* Configure ODK Central base URL (url), project ID (pid), and form ID (fid) 
in one step using the OData Service URL.
* Paste your own Service URL over `svc="..."`.
* Paste your local timezone over `Australia/Perth`.
* Adjust `loc` to a local subfolder for downloaded attachments.
-->
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
# Data wrangling
library(dplyr)
library(fs)
library(ruODK)

# Visualisation
library(skimr)
library(knitr)
library(reactable)
library(leaflet)

# Dissemination
library(readr)
# library(ckanr)
# library(googledrive)

# Spatial
# library(stringr)
# library(rgeos)
# library(sf)

# Set ruODK defaults to an ODK Central form, choose tz and verbosity
ruODK::ru_setup(
  svc=paste0("https://odkcentral.dbca.wa.gov.au/v1/projects/3",
             "/forms/build_Flora-Quadrat-0-4_1586245110.svc"),
  tz = "Australia/Perth",
  verbose = TRUE)

loc <- fs::path("media")
fs::dir_create(loc)
```

# Download data
<!-- Get table names. `ft` will contain one row per separate table. -->
```{r svc_get}
ft <- ruODK::odata_service_get()
ft %>% knitr::kable(.)
```

<!-- 
Get and parse submissions.

Repeat for each table name in form_tables$url. 

Once happy with the parsing results, verbose can be set to FALSE.

A note on geofields:

Points (geopoint), Lines (geotrace), Polygons (geoshape) parse with either
GeoJSON or WKT. If you want to further spatially enable the data, choose
via parameter `wkt` the format that works best for you.

`wkt=TRUE` returns strings of WKT which you can parse with e.g. 

wkt_geoshape %>%
    stringr::str_replace_all(",undefined NaN", "") %>%
    rgeos::readWKT() %>%
    sf::st_as_sfc()
    
`wkt=FALSE` returns GeoJSON
-->
```{r dl_submissions}
data <- ruODK::odata_submission_get(
  table = ft$url[1], 
  local_dir = loc, 
  wkt = TRUE
)

data_sub1 <- ruODK::odata_submission_get(
  table = ft$url[2], 
  local_dir = loc
) %>%
  dplyr::left_join(
    data, 
    by = c("submissions_id" = "id")
  )

data_sub2 <- ruODK::odata_submission_get(
  table = ft$url[3], 
  local_dir = loc
) %>%
  dplyr::left_join(
    data, 
    by = c("submissions_id" = "id")
)

# repeat for any remaining nested tables
```

# Analyse and visualise data
<!-- Summarise, analyse, visualise the tibble(s) data (, data_sub1, data_sub2) -->
```{r data_vis}
skimr::skim(data)
dplyr::glimpse(data)
reactable::reactable(data, filterable = TRUE, sortable = TRUE)
```

```{r data_map}
leaflet::leaflet(width = 800, height = 600) %>%
  leaflet::addProviderTiles("OpenStreetMap.Mapnik", group = "Place names") %>%
  leaflet::addProviderTiles("Esri.WorldImagery", group = "Aerial") %>%
  leaflet::clearBounds() %>%
  leaflet::addAwesomeMarkers(
    data = data,
    # 
    # # Adjust to your coordinate field names
    # 
    lng = ~location_corner1_longitude, 
    lat = ~location_corner1_latitude, 
    icon = leaflet::makeAwesomeIcon(text = "Q", markerColor = "red"),
    # 
    # # With your own field names
    # 
    label = ~ glue::glue("{location_area_name} {encounter_start_datetime}"),
    popup = ~ glue::glue(
      "<h3>{location_area_name}</h3>",
      "Survey start {encounter_start_datetime}</br>",
      "Reporter {reporter}</br>",
      "Device {device_id}</br>",
      "<h5>Site</h5>",
      '<div><img src="{habitat_morphological_type_photo}"',
      ' height="150px" alt="My photo"></img></div>'
    ),
    clusterOptions = leaflet::markerClusterOptions()
  ) %>%
  leaflet::addLayersControl(
    baseGroups = c("Place names", "Aerial"),
    options = leaflet::layersControlOptions(collapsed = FALSE)
  )
```

# Export
<!--
The form submissions are now extracted and visualised. What's next:

* Save data to local files (e.g. CSV).
* Compile report (e.g. to HTML).
* Compress all outputs as ZIP.
* Upload these artifacts to a CKAN data catalogue.
* Upload same artifacts to Google Drive.

Notes: 

* Generate the HTML report once off without the next chunk (`eval=F`), 
as the chunk refers to the rendered output file (HTML) before the file is 
created initially.
* Run report always twice to generate (run 1) and upload (run 2) the latest HTML.
-->
```{r data_export, eval=FALSE}
#------------------------------------------------------------------------------#
# Prepare report and products as local files
#
rep_fn <- "FloraQuadrats04.html" # The file name you save this template under
data_fn <- here::here("fq.csv") %>% as.character()           # Main data
data_sub1_fn <- here::here("fq_strata.csv") %>% as.character() # Sub table 1
data_sub2_fn <- here::here("fq_taxa.csv") %>% as.character() # Sub table 2
zip_fn <- "products.zip" # Attachments as one zip file (top level)

# Write data tbls to CSV files
readr::write_csv(data, path = data_fn)
readr::write_csv(data_sub1, path = data_sub1_fn)
readr::write_csv(data_sub2, path = data_sub2_fn)

# Compress everything into `zip_fn`, retain relative path to `loc`
# zip(zipfile = zip_fn, files = fs::dir_ls(loc))

#------------------------------------------------------------------------------#
# CKAN
#
# Upload to a CKAN data catalogue (need url and API key of a write permitted user)
# See ROpenSci package ckanr
ckanr::ckanr_setup(url = Sys.getenv("CKAN_URL"), key = Sys.getenv("CKAN_KEY"))
ckan_ds_name <- "edc-flora-quadrats"

# Run once to create resources on an existing dataset, then comment out
d <- ckanr::package_show(ckan_ds_name)
res_data_main <- ckanr::resource_create(
  package_id = d$id, name="Main data", upload = data_fn)
res_data_sub1 <- ckanr::resource_create(
  package_id = d$id, name="Nested data table 1", upload = data_sub1_fn)
res_data_sub2 <- ckanr::resource_create(
  package_id = d$id, name="Nested data table 2", upload = data_sub2_fn)
res_report <- ckanr::resource_create(
  package_id = d$id, name="Data report", upload = rep_fn)
res_zip <- ckanr::resource_create(
  package_id = d$id, name="All data and attachments", upload = zip_fn)

# Paste res_data_main$id over RID and keep here, repeat for each resource
r <- ckanr::resource_update(res_data_main$id, path = data_fn)
r <- ckanr::resource_update(res_data_sub1$id, path = data_sub1_fn)
r <- ckanr::resource_update(res_data_sub2$id, path = data_sub2_fn)
r <- ckanr::resource_update(res_report$id, path = res_report)
r <- ckanr::resource_update(res_zip$id, path = zip_fn)

#------------------------------------------------------------------------------#
# Google Drive
#
# Run once per machine, then comment out:
googledrive::drive_auth(use_oob = TRUE)

# Upload to Google Drive
gd_fn <- "My Google Drive folder name"
googledrive::drive_ls(gd_fn) %>% googledrive::drive_rm(.)  # Wipe older outputs
googledrive::drive_upload(rep_fn, path=rep_fn)             # Report as HTML
googledrive::drive_upload(data_fn, path=data_fn)           # Main data as CSV
googledrive::drive_upload(data_sub1_fn, path=data_sub1_fn) # Sub table 1 as CSV
googledrive::drive_upload(data_sub2_fn, path=data_sub2_fn) # Sub table 2 as CSV
googledrive::drive_upload(zip_fn, path=zip_fn)             # All outputs as ZIP
```
